{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Once', 'upon', 'a', 'time']\n",
      "Token IDs: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Simple tokenization by splitting the text into words\n",
    "tokens = prompt.split()\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Define a simple vocabulary\n",
    "vocab = {\n",
    "    \"Once\": 0,\n",
    "    \"upon\": 1,\n",
    "    \"a\": 2,\n",
    "    \"time\": 3\n",
    "}\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = [vocab[token] for token in tokens]\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings:\n",
      " [[0.06377145 0.3878672  0.08188198 0.21533448 0.40588766 0.46398555\n",
      "  0.05341908 0.85256056]\n",
      " [0.31828137 0.31786979 0.18692775 0.92403407 0.72886036 0.75928042\n",
      "  0.13488984 0.49474832]\n",
      " [0.40731337 0.11683353 0.15344824 0.83145745 0.65755591 0.45402358\n",
      "  0.72777958 0.34526426]\n",
      " [0.08423454 0.94702216 0.91144005 0.96876322 0.40983828 0.4385876\n",
      "  0.40125965 0.81762265]]\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding matrix (for simplicity, using random initialization)\n",
    "embedding_dim = 8  # Dimension of the embedding vectors\n",
    "vocab_size = len(vocab)\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert token IDs to embeddings\n",
    "embeddings = np.array([embedding_matrix[token_id] for token_id in token_ids])\n",
    "print(\"Embeddings:\\n\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06377145, 0.3878672 , 0.08188198, 0.21533448, 0.40588766,\n",
       "       0.46398555, 0.05341908, 0.85256056])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Embeddings with Positional Encodings:\n",
      " [[ 0.06377145  1.3878672   0.08188198  1.21533448  0.40588766  1.46398555\n",
      "   0.05341908  1.85256056]\n",
      " [ 1.15975236  0.8581721   0.19692758  1.92398407  0.72896036  1.75928041\n",
      "   0.13489084  1.49474832]\n",
      " [ 1.3166108  -0.2993133   0.17344691  1.83125745  0.65775591  1.45402356\n",
      "   0.72778158  1.34526426]\n",
      " [ 0.22535454 -0.04297034  0.94143555  1.96831325  0.41013828  1.43858755\n",
      "   0.40126265  1.81762265]]\n"
     ]
    }
   ],
   "source": [
    "# Function to get positional encodings\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    positional_encoding = np.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
    "    return positional_encoding\n",
    "\n",
    "# Get positional encodings\n",
    "positional_encodings = get_positional_encoding(len(token_ids), embedding_dim)\n",
    "\n",
    "# Add positional encodings to embeddings\n",
    "final_embeddings = embeddings + positional_encodings\n",
    "print(\"Final Embeddings with Positional Encodings:\\n\", final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[6.43078014 5.64968091 3.0423906  2.50005347 3.537634   3.14750561\n",
      "  4.57006251 3.87397084]\n",
      " [6.44167115 5.66056728 3.04814643 2.50455417 3.54761774 3.14936102\n",
      "  4.571603   3.87610748]\n",
      " [6.43810953 5.65702969 3.04639234 2.50305936 3.54440886 3.14881096\n",
      "  4.57106977 3.87532509]\n",
      " [6.43512108 5.65407702 3.04492533 2.50183819 3.54169492 3.14843514\n",
      "  4.57068588 3.87471638]]\n"
     ]
    }
   ],
   "source": [
    "# Define the attention mechanism\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def attention(query, key, value, mask=None):\n",
    "    scores = np.dot(query, key.T) / np.sqrt(query.shape[-1])\n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "    weights = softmax(scores)\n",
    "    return np.dot(weights, value)\n",
    "\n",
    "# Example weights for the attention mechanism (random initialization for simplicity)\n",
    "weights_query = np.random.rand(embedding_dim, embedding_dim)\n",
    "weights_key = np.random.rand(embedding_dim, embedding_dim)\n",
    "weights_value = np.random.rand(embedding_dim, embedding_dim)\n",
    "\n",
    "# Create query, key, and value matrices\n",
    "query = np.dot(final_embeddings, weights_query)\n",
    "key = np.dot(final_embeddings, weights_key)\n",
    "value = np.dot(final_embeddings, weights_value)\n",
    "\n",
    "# Apply the attention mechanism\n",
    "attention_output = attention(query, key, value)\n",
    "print(\"Attention Output:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
